\chapter{The ScienceIE Task: Evaluation}

\begin{table}
	\centering
	\begin{tabular}{ c | C{2.7cm} | C{2.7cm} | C{2.2cm} | C{2.2cm} }
		\multirow{2}{*}{\textbf{Subtask}} &\multicolumn{2}{c|}{\textbf{ScienceIE}} &\multicolumn{2}{c}{\textbf{This Report}} \\
		& \textbf{Individual \newline Best / Average} & \textbf{End-To-End \newline Best / Average} & \textbf{Individual \newline Best} & \textbf{End-To-End \newline Best} \\
		\hline
		KP Extraction & 0.56 / 0.38 & 0.56 / 0.38 & 0.2 & 0.2 \\
		KP Classification & 0.67 / 0.57 & 0.44 / 0.26 & 0.55 & 0.11 \\
		Relation extraction & 0.64 / 0.43 & 0.28 / 0.07 & 0.1 & 0.02 \\
		Overall & N/A & 0.43 / 0.25 & N/A & 0.11    
	\end{tabular}
	\caption[Summary of Results Evaluated With ScienceIE Scripts]{The best F1 scores achieved in this paper evaluated with the supplied ScienceIE scripts. This includes both tests for end-to-end data production, and individual subtask tests (where the gold standard data from the previous subtask is fed in). Summarised ScienceIE results are also included, extracted from the ScienceIE proceedings\cite{Augenstein2017}.}
	\label{table:scienceieresults}
\end{table}

With solution systems proposed for the various subtasks of ScienceIE, each were given data to train on and predict on. Below are results where the algorithms have been tested end-to-end (where they are run through in order, with the data from one being passed to the next) and independently (where they are given the gold data as a starting point, and operate on that information). There is also some self evaluation conducted, to explore the effectiveness of evaluating under conditions of varying strictness.

A summary of evaluation using the ScienceIE scripts, which covers all subtasks using the best algorithms found within this paper, and also includes the ScienceIE scores for comparison, is presented in table \ref{table:scienceieresults}. These will be discussed throughout this section.

\section{Subtask A - Key Phrase Extraction}

\subsection{Method 1: Support Vector Machine}
As described in the design section of this report, the SVM created for extracting key phrases began with a set of features which then underwent several iterations with other features being added.

A problem referenced in the literature review was evaluation of key phrase extraction. What was discussed is how to classify good and bad systems. It is easy to accurately say how well a system performed compared to a set of gold standard data, but this paper considers whether that is enough to say whether a given extraction system is a success or not.

% TODO Maybe talk abbout existing solutions in cross bracket metrics

\begin{table}
	\centering
	\begin{tabular}{ C{3cm} | C{10cm} }
		\textbf{Strictness} & \textbf{Description} \\
		\hline
		Very strict (\texttt{REALLY\_STRICT}) & Both the predicted key phrase string and its boundaries need to match the gold standard string and boundaries. This is of equivalent strictness to the \textit{BRAT} evaluation completed by the ScienceIE scripts. \\
		\hline
		Matching (\texttt{STRICT}) & Just the predicted key phrase string needs to match the gold standard string. \\
		\hline
		Inclusive matching (\texttt{INCLUSIVE}) & The predicted key phrase string must be equal too or include the gold standard string. \\
		\hline
		Generous matching (\texttt{GENEROUS}) & The predicted key phrase string can include the gold standard string, or the gold stand string can include the predicted. \\
	\end{tabular}
	\caption[Strictness Descriptions for Self Evaluation of Key Phrases]{The strictness levels used in self evaluation of key phrase extraction. The \textit{strictness} title also includes the \texttt{Strictness} enumeration value as in the Java code for reference.}
	\label{table:strictness}
\end{table}

Therefore, when conducting self evaluation, several metrics will be generated. The metric titles and descriptions can be seen in table \ref{table:strictness}.

\subsubsection{Experimenting With Longer Documents}

\subsection{Method 2: Clustering}
While the SVM method showed some good results, unfortunately clustering was not as successful. 

A problem encountered during development was that not every word was in the Word2Vec vocabulary, meaning not every word could be clustered. This is actually quite a major problem as, given the context of the words we'd like to cluster is scientific papers, many of these unusual words not in the corpus are likely part of the key words describing the key pieces of information in the paper. Given we want to be able to cluster based on those terms to find them in future papers to extract as key phrases, this limits the capability and robustness of the algorithm. The only thing that could be done was to continue clustering until we are left with one cluster with everything that can be clustered in, and a group of clusters with one element each, where those singular terms are not in the vocabulary of Word2Vec. This problem didn't occur for every test paper processed, but has shown some papers to have over 20 tokens that were excluded from the Word2Vec model. 

Next came the problem of how the clusters formed. Ideally, we would like to see the case where, for many iterations, many clusters are present with many items each. This could then be used to split the document into many clusters with a (for instance) key phrase in each, taking the tokens at the centre of each cluster at a given level to be part of its own key phrase. What was generated instead was a pattern where, generally, two items clustered together which were there joined by a third item on the next iteration, then a fourth on the next, and so on, while no other tokens clustered together in other clusters. This is significantly less useful, as a bunch of words are grouped together in a growing group, while all others are left on their own until they are included in this large group. Very occasionally, other clusters grew to have a few items rather than immediately join the large cluster, but this was rare to see and still not very useful to draw multiple key phrases from.

To aid in the understanding of what happened, an illustration is provided in figure \ref{figure:kpclusteringeg}. The left drawing is the ideal case, and the right is what commonly occurred. It didn't necessarily have to happen in token order, but the point is that aside from picking tokens that are very close together, this does not give us much insight into the key topics of the paper.

\begin{figure}
	\centering
	\begin{tikzpicture}[nodes={draw, circle}]
	\node (a) at (-4, 0) {a};
	\node (b) at (-3, 0) {b};
	\node (c) at (-2, 0) {c};
	\node (d) at (-1, 0) {d};
	\node (e) at (0, 0) {e};
	\node (ab) at (-3.5, 3) {};
	\node (cd) at (-1.5, 1) {};
	\node (cde) at (-0.75, 2) {};
	\node (all) at (-2.125, 5) {};
	
	\draw  (a) |- (ab);
	\draw  (b) |- (ab);
	\draw  (c) |- (cd);
	\draw  (d) |- (cd);
	\draw  (e) |- (cde);
	\draw  (cd.center) |- (cde);
	\draw  (ab.center) |- (all);
	\draw  (cde.center) |- (all);
	
	\node (a2) at (2 ,0) {a};
	\node (b2) at (3, 0) {b};
	\node (c2) at (4, 0) {c};
	\node (d2) at (5, 0) {d};
	\node (e2) at (6, 0) {e};
	\node (ab2) at (2.5, 1) {};
	\node (abc2) at (3.25, 2) {};
	\node (abcd2) at (4.125, 3) {};
	\node (all2) at (5.0625, 5) {};
	
	\draw  (a2) |- (ab2);
	\draw  (b2) |- (ab2);
	\draw  (c2) |- (abc2);
	\draw  (d2) |- (abcd2);
	\draw  (e2) |- (all2);
	\draw  (ab2.center) |- (abc2);
	\draw  (abc2.center) |- (abcd2);
	\draw  (abcd2.center) |- (all2);
	\end{tikzpicture}
	\caption[Visual Representation of Key Phrase Clustering]{Examples of hierarchical clustering on a set of items, \textit{a}, \textit{b}, \textit{c}, \textit{d} and \textit{e}, through to when they are all in the same cluster. The left drawing is an example of ideally what we would like to have seen. The right is a representation of what was very common generated.}
	\label{figure:kpclusteringeg}
\end{figure}

\section{Subtask B - Key Phrase Classification}

\begin{table}
	\centering
	\begin{tabular}{ C{2.5cm} | C{2cm} | C{2cm} | C{2cm} | C{2cm} | C{2cm} }
		\textbf{Word2Vec Model} & \textbf{Distance Metric} & \textbf{Default Class} & \textbf{Remove words?} & \textbf{Use many words?} & \textbf{Accuracy} \\
		\hline
		Google News & Average & Unknown & No & No & \textbf{47.90\%} \\
		Google News & Average & Unknown & Yes & No & \textbf{47.76\%} \\
		Google News & Average & Unknown & No & Yes & \textbf{45.47\%} \\
		Google News & Average & Unknown & Yes & Yes & \textbf{45.47\%} \\
		\textbf{Google News} & \textbf{Average} & \textbf{Material} & \textbf{No} & \textbf{No} & \textbf{54.58}\% \\
		Google News & Average & Material & Yes & No & \textbf{54.43\%} \\
		Google News & Average & Material & No & Yes & \textbf{52.14\%} \\
		Google News & Average & Material & Yes & Yes  & \textbf{52.14\%} \\
		Google News & Closest & Unknown & No & No & \textbf{46.05}\% \\
		Google News & Closest & Unknown & Yes & No & \textbf{45.96\%} \\
		Google News & Closest & Unknown & No & Yes & 44.05\% \\
		Google News & Closest & Unknown & Yes & Yes & 43.91\% \\
		Google News & Closest & Material & No & No & \textbf{52.73\%} \\
		Google News & Closest & Material & Yes & No & \textbf{52.63\%} \\
		Google News & Closest & Material & No & Yes & \textbf{50.73}\% \\
		Google News & Closest & Material & Yes & Yes & \textbf{50.58\%} \\
		Freebase & N/A & Unknown & N/A & N/A & 0.00\% \\
		Freebase & N/A & Material & N/A & N/A & 44.05\% \\
	\end{tabular}
	\caption[Word2Vec Classification Results]{The above table is the various configurations of the Word2Vec classifier, running with every possible configuration for the five parameters are listed. The result in bold line is the highest scoring configuration, with bold results being those above the baseline score of 44.05\% which is where every key phrase is simply classified as a \textit{material}. All Freebase results were not listed as there was no change in result other than for the \textit{default class} variable, so \textit{N/A} is present instead of each iteration of those variables. These results are based on classifying all 2052 ScienceIE key phrase test data points.}
	\label{table:classresults}
\end{table}

Key phrase classification under the outlined Word2Vec classifier was quick to execute. After the initial, one time per system boot, wait for the Word2Vec model to be loaded into memory, the actual calculations required are very fast to execute, giving this classifier a very high throughput.

In terms of result, it can be seen that several configurations allow for over 50\% of the key phrases to be classified correctly. The full range of results for each configuration can be seen on table \ref{table:classresults}.

The obvious thing to note here is that all tests under Freebase without a default class have a 0\% accuracy (i.e. it didn't classify any key phrase correctly). With a little investigation, if all key phrases were labelled as a \textit{material}, the accuracy would be 44\% - which is what was achieved by all configurations with the Freebase model and a default classification of \textit{material}. Therefore, Freebase clearly isn't an effective vocabulary for working with scientific publications, seemingly containing one of the supplied words. 

When using the Google News model, things are less clear. Generally, using this model a decent accuracy can be achieved, even without the support of a default classification. This shows the model does include many of the terms used in the publications presented. In terms of distance metric, \textit{average} was slightly better; it was part of the best configuration (scoring 54.6\%), and also the average of all of the \textit{average} distance configurations was 2\% higher than the average of the \textit{closest} configurations (50\% and 48\% respectively).

The configuration change that made the least difference was whether or not the unimportant and stop words were removed. This saw changes, averagely, of 0.1\% when comparing two configurations differing in only whether these words were removed. When some words were not removed the algorithm performed better, indicating that these words are more important than first thought for deciding class. This may be down to the edge cases discussed such as "He" or similar which, despite being a stop word, we would want to keep.

Using various words when finding similarity decreased the accuracy of classification, and was the largest detrimental parameter. When using many words to find similarity, accuracy decreased an average of 2.2\% (50.3\% down to 48.1\%). This means the position of the target classes in the Word2Vec model are likely quite accurate when compared to words that can be used in a similar context.

Using the best configuration found my testing within this paper, evaluating this classifier through the ScienceIE scripts gives an F1 score of 0.55 when evaluated individually (given the gold standard key phrases). This isn't a bad score, as it almost matches the average ScienceIE F1 score for this section done independently.

\subsection*{Other Experimentation}
While working on key phrase classification, having seen some decent results produced by the SVM created in subtask 1, a short experiment was conducted to see how well using position in a document could be used to determine classification.

The process of trying this was very simple: re-purpose the subtask A SVM (with all described extensions included) code to, when training, label data according to a class rather than to key phrase. Two versions of this were attempted:

\begin{itemize}
	\item The SVM is given 4 different labels, one for each classification and one for no classification (i.e. not part of a key phrase).
	\item 3 different SVMs were trained, each only having one classification labelled.
\end{itemize}

Given the only real semantic meaning for a token in the SVM was based around whether the token was a noun, this was not expected to perform well as semantics and the specific words surrounding what we are trying to classify generally are expected to be important, rather than the position of the token in the text which gives us little context.

As expected, results were terrible, with F1 scores for both versions being less than 0.1. This confirms that the position of tokens, at least as far as this paper's positional information is concerned, is not useful in classifying tokens. It may have been interesting to try again leaving out non key phrase tokens from the training data generated for the SVM to learn from, but these low results didn't encourage further study into this idea when other, more efficient and productive concepts were being worked on.

\section{Subtask C - Relation Extraction}

Both SVMs concepts were tested and the results for the hyponym and synonym variants combined for full evaluation.

\begin{table}
	\centering
	\begin{tabular}{c | c | c | c | c }
		\textbf{SVM Model} & \textbf{Vector Generation} & \textbf{True Positives} & \textbf{False Positives} & \textbf{F1 Score} \\
		\hline
		\textit{Many Features} & All tokens included & 13 & 84 & \textbf{0.09} \\
		\textit{Many Features} & Root noun selected & 2 & 28 & 0.02 \\
		\textit{Many Features} & Unimportant words removed & 0 & 0 & 0 \\
		\textit{Few Features} & All tokens included & 0 & 0 & 0 \\
	\end{tabular}
	\caption[Relation Extraction Specific Results]{Results from doing relation extraction with the proposed SVMs. The results are calculated after the relations extracted by the hyponym and synonym SVMs are combined. The \textit{vector generation} column described how a vector for a key phrase was found (for example, where the unimportant words are removed). The bold result is the highest F1 score achieved.}
	\label{table:relresults}
\end{table}

As presented in summary table \ref{table:scienceieresults}, the best overall results for this synonym extraction was an F1 score of 0.1\%. This was from using the \textit{many features} SVM with the vectors from each token in a key phrase being used to calculate a key phrases total vector. A more detailed breakdown of how well each configuration performed can be seen in table \ref{table:relresults}. 

From this table is it clear to see that the \textit{many features} SVM concept performed better than the \textit{few features}. While the hope was to allow the SVM to work on information with a lower complexity to help it find a better fitting hyperplane, this clearly hasn't worked as it didn't pick any relations at all; compared to \textit{many features} with the same vector generation, which chose a total of 101 relations. 

In terms of the different methods of generating vectors for key phrases, keeping all of the information in the key phrase appears to be the best method. Most true positives where found when all tokens are kept, while less true positives came from where the root nouns are selected, and no true positives were found when stop words and unimportant words were removed. This mirrors what happened in subtask B, where potentially unimportant information was removed and a decrease in the quality of results is observed; although it is more noticeable here given it is the difference between some and no relations being extracted (there was only a 0.1\% change in subtask B's results).

Furthermore, while leaving more important information in the key phrase for vector generation did improve the system performance, the false positives increases a large amount as well. Selecting just the root noun saw 14 times more false positive results than true positive; compared to when all key phrase information was left in, only 6 times as many false positives to true positives were seen. Not excluding data clearly improves the system accuracy, and increases the F1 score.

\section{Conclusion}

