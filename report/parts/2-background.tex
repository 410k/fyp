\section{Background and Literature Review}

Evaluating the outcome of ScienceIE at SemEval indicates potential paths for future systems and document very recent activity in the key phrase extraction area. Three papers were published from the event regarding this task.

One team’s results \cite{Marsi2017} included a summary of all participating teams attempts, which detailed the highest F1 score measured at 0.43 for all three sub-systems combined, which can be seen as a potential target for this project (at least to get close to matching and potentially surpassing it). Furthermore, when deciding the algorithms to be used in the system created, only 31\% of all keywords found had been seen in training, meaning any system created must be generic to extract key phrases from future unseen papers. For feature extraction, it was evaluated that while many high scores were achieved with recurrent neural networks (NN), the highest scoring system was a support vector machine (SVM) using a well-engineered lexical feature set. SVMs and NNs were also popular choices for key phrase classification. For relation extraction, many methods were attempted and while a convolutional NN was the most effective, various other methods (including SVM, multinomial naïve Bayes and more) all achieved very similar and reasonably accurate scores (up to F1 0.54).

The best end-to-end ScienceIE team used a long short-term memory (LSTM) approach for phrase extraction, with labelling completed by a conditional random field (CRF) based sequence tagging model \cite{Ammar2017}. Their sequence tagging model employed gazetteers built from scientific words extracted from the web. Another team \cite{Marsi2017} also had similar ideas, using CRFs to complete some of the task using WordNet\footnote{https://wordnet.princeton.edu/} as a data source for the classifier they created. Both teams here also used sensible rules to help improve their score, such as intuitively marking all instances of a key phrase as a key phrase upon finding one instance (so if ‘carbon’ is extracted and labelled as a ‘material’, then all other instances are labelled to match) and exploiting hypernym relationship’s bidirectional property (so if word 1 is a hypernym of word 2, the reverse is also true and therefore recorded). 

Unfortunately, while extraction and classification were generally well handled, relation extraction has very low accuracy across all teams taking part with the average F1 score only being 0.15, with the highest score being 0.28. \cite{Ammar2017} achieved this using gazetteer built from Wikipedia\footnote{https://en.wikipedia.org} and freebase. This seems more appropriate than hand written rules, which may seem appealing as they can be somewhat tailored and provide high accuracy, but require much more effort from the developer and may not work well on unseen conditions providing low accuracy \cite{Manning2012}. As mentioned earlier, WordNet is also a potential source of information for building a classifier for relation extraction, and a study by \cite{Snow2013} compared building a classifier off of Wordnet and Wikipedia for hypernym-only extraction. The result of this shows that Wikipedia may be more suited to creating this type of classifier as it achieved an F1 score higher than using WordNet (the Wikipedia based classifier got 0.36 while the WordNet based classifier got 0.27). While an improvement, it is not ultimately a huge increase and there is no evidence either Wikipedia is better for the specific area of scientific papers (as the 2013 study was completed on a generic set of data).

The results of ScienceIE demonstrate there are several potential systems that could be implemented to answer this problem, with the best system potentially being a combination of algorithms and a voting system to select and label key phrases. The product would likely involve supervised learning and previous knowledge for some algorithms, along with unsupervised learning sections as well.

On the note of combining systems, several teams chose to back up key phrase extraction with a CRF. CRFs can be used for key phrase extraction alone as well, as documented by Zhang et al. \cite{Zhang2008}, and while this study implies that CRFs (with an F1 of 0.51) are more accurate than an SVM (the most accurate at ScienceIE) this paper is slightly older than papers produced at SemEval 2017 and so even if the SVM information used then was the best that was available at the time (the SVM F1 score was 0.46), the SVM implemented at ScienceIE beat both of these scores considerably achieving an F1 of 0.56 \cite{Augenstein2017}.

A method not attempted at ScienceIE was unsupervised learning by clustering key phrases, a method which has potentially very accurate results that also could not only be robust again new unseen data but even different languages. The idea is that candidate key phrases are selected by some heuristic and other phrases are clustered about them. With the simplest approach, the center of a cluster is the key phrase. Various clustering methods were attempted by \cite{Liu2009} on top of a candidate selection process built on semantic term relatedness. They ran tests on relatively short articles and while at maximum they only achieved an F1 of ~0.45, there was several improvements suggested which apply to the task at hand concerning scientific papers. Firstly, an achievable improvement for this project would be to cluster directly on noun groups as they found most clusters consisted of groups of nouns anyway, which is backed up by Augenstein et al. \cite{Augenstein2017}, who reports 93\% of all key phrases are noun phrases. Furthermore, improving their initial filtering to extend it further than stop words may help reduce errors as well; improving this may be possible by employing a words TF-IDF score with some threshold. Finally, they suggested a similar algorithm be applied to longer scientific papers. ScienceIE’s test data consists of extracts of scientific texts (i.e. short paragraphs), however, any unsupervised system created for this task could be ran again entire papers and then only those sections compared for evaluation later – allowing this suggestion to be evaluated.
