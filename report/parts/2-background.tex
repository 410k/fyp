\chapter{Background and Literature Review}

\section{Definitions and Descriptions}
Throughout this literature review there are several key natural language processing and machine learning concepts discussed. Rather than defining them as we arrive at them, a list of useful definitions is constructed here.

\subsection*{F1 Score}
The F1 score is a metric used to evaluate predictions. A common concept to find whe evaluating binary decisions is a \textit{confusion matrix} from which ROC analysis can be completed \cite{Fawcett2006}. This is a system which records \textit{true positive} and \textit{true negative} where the gold standard and predicted data match, and \textit{false positive} and \textit{false negative} where gold and predicted data do not match. From this, various values can be calculated. 

Accuracy is one, but often doesn't show the full story as if, on a data set where 9 out of 10 items are 'false' and just 1 item is 'true', predicting all false will get an accuracy of 90\%, but no \textit{true positive} occurrences will appear - which is bad. 

Two better metrics can be calculated, which are precision and recall. They look at the rates of correct and incorrect predictions, and can be combined together (and often are in the NLP world) to produce an F1 score. This is what ScienceIE's scripts calculate, given ScienceIE's gold standard data and a researchers predictions, comparing instances where the researcher has correctly predicted key phrase boundaries, classification and relations against the gold standard data.

An F1 score of 1 is perfect, and an F1 score of 0 means nothing was classified correctly at all.

\subsection*{Tokenization}
Tokenizatoin is a simple concept where a document is broken down, from one long string into individual words or symbols. 

\subsection*{Bag-Of-Words Representation}
Bag-of-Words is another simple concept, where each token is considered independently of is semantic meaning. 

\subsection*{Stop Words}
Stop words are words that are commonly filtered out due to their lack of specific meaning and generally do not contribute to useful input, although exceptions can be made if there is little other information. For example, common search engines are likely to ignore the word "the" in most searches, but this process of removal needs to be conducted carefully as, for example, when searching for "the who", "the" is an important part of that query. 

The stop words concept is used in this report, and the list of stop words used is taken from the Stanford CoreNLP GitHub repository\footnote{\href{https://github.com/stanfordnlp/CoreNLP/blob/master/data/edu/stanford/nlp/patterns/surface/stopwords.txt}{https://github.com/stanfordnlp/CoreNLP/blob/master/data/edu/stanford/nlp/patterns/surface/stopwords.txt}} and shown in appendix \ref{appendix:stopwords}.

\subsection*{Term Frequency - Inverse Document Frequency (TF-IDF)}
TF-IDF is a metric for assessing how important a word is in a piece of text and has many applications in NLP, including (which will be discussed later) document query \cite{Ramos2003}. It shall also see use throughout this report.

To be calculated for a given token, the document that token came from is required and a set of documents for comparison is required. The set of documents used in this project shall be the ScienceIE training set.

The theory is, a word like "the" - which will likely appear many times in almost every document - should a very low TF-IDF score (close to 0). Unusual words however, such as "xylanases" which are likely very specific to the paper they are contained in will probably have a much higher TF-IDF value than that of "the".

\subsection*{Parse Trees and Part-Of-Speech (POS) Tagging}
A \textit{parse} of a sentence is a tree structure where the root node is a \textit{sentence}, each node below that is a \textit{POS tag} and the leaves of the tree are the words or symbols in the sentence. The POS tag tells us about the semantic meaning of that node and its children - or sub tree - where the POS tag could be \textit{verb phrase} or \textit{noun phrase} for example.

\subsection*{Support Vector Machine (SVM)}
A SVM is a supervised machine learning mechanism. The input to a SVM is a series of vectors generated from some original input data. Each of these vectors is a set of features - which are simply values calculated based on the original input data. For training, these data points are labelled, indicating their class. Once trained, a SVM can be used to \textit{predict} the label for a new data point.
% todo a refernce here and improve in general

The training involves attempting to find a well fitting hyperplane with a maximal margin, that separates the labelled data, after mapping that data into a higher dimensional space. This involves an algorithm for finding the distance between the mapped data points, for which a \textit{kernel} can be specified. Furthermore, the hyperplane that is fitted can be allowed to make errors. This is where it allows training data points to be within the hyperplane margins (so may be miss classified if tested against). This can be tuned to increase SVM performance at the cost of run time increasing as well.

\subsection*{Clustering}
Clustering is simply the idea of grouping items together that are similar. The result should be a set of sets of items, where within the group there is a high average similarity, while inter-group similarity is much lower. This is often used for classification and there are a variety of clustering algorithms that can be applied, with various algorithms performing better for various applications \cite{Rai2010}.

\section{ScienceIE Proceedings}
Evaluating the outcome of ScienceIE at SemEval indicates potential paths for future systems and documents very recent activity in the information and key phrase extraction area. Three papers were published from the event regarding this task.

Firstly, an overview of how successful the task was shall be conducted. The highest end-to-end F1 score achieved by any team was measured to be 0.43 for all three sub-systems combined, with each of subtasks A, B and C were 0.56, 0.44 and 0.28 respectively (for end-to-end tests) \cite{Augenstein2017}. 

For subtask A, it was evaluated that while many high scores were achieved with recurrent neural networks, the highest scoring system was a SVM using a well-engineered lexical feature set. SVMs and neural networks were also popular choices for subtask B. For subtask C, many methods were attempted and while a convolutional neural network was the most effective, various other methods (including SVM, multinomial naïve Bayes and Conditional Random Fields (CRFs)) all achieved very similar and reasonably accurate scores (the best had an F1 score of 0.64 when evaluated solely on subtask C).

Furthermore, a common preprocessing technique was to use the spaCy NLP pipeline to analyse the given texts to achieve knowledge about their semantics \cite{Honnibal2015}. Then, the key phrases were directly annotated on to the text model with a flavour of tagging scheme (typically Inside-Outside-Begin or Begin-Inside-Last-Outside-Unit) so that algorithms operating on the produced models have the annotation information to learn from. Algorithms would then predict these tags on the test data, which would then be post processed into the BRAT annotation format for evaluation.

The best end-to-end ScienceIE team applied sequence tagging models, which in tern used long short-term memory (LSTM) neural networks and CRFs to solve each subtask separately \cite{Ammar2017}. Their sequence tagging models also employed gazetteers built from scientific words extracted Wikipedia and Freebase, which appear to perform suitably in the context of scientific papers, as their high results (scoring first or second in all of the scenarios evaluated) indicate the vocabulary supported by these two resources covered much of the test data (their algorithms would likely have not scored so high if a large portion of the vocabulary was missing from the gazetteers created).

Another contribution also included the use of CRF based models \cite{Marsi2017}. This team actually created a range of CRFs, each with a different intention. They built a range of models, each targeting a specific goal (general key word extraction, task key word extraction, task classification and so on), and each with their own set of features chosen through cross validation. These features generally focussed on the format of the word, the position of the word, and (for material key phrases) comparison to known hyponyms, using WordNet\footnote{\href{https://wordnet.princeton.edu/}{https://wordnet.princeton.edu/}} as a data source. A problem encountered here was that they found some classes had few examples given, so as a solution they removed all sentences that didn't contain a given class (to only focus on areas with positive examples to help balance class distributions). Finally, as there were overlap in the CRF classifiers created, they could be run in parallel and a voting system used to choose a final prediction.

Both of the solution sets above also used sensible rules to help improve their score, such as intuitively marking all instances of a key phrase as a key phrase upon finding one instance (so if \textit{carbon} is extracted and labelled as a \textit{material}, then all other instances in the same document are labelled to match). The teams also exploited hyponym relationship’s bidirectional property (so if word 1 is a hyponym of word 2, word 2 is a hypernym of word 1), meaning their classifier could classify the relation in either direction and the final \textit{hyponym-of} relation could be returned.

The results of ScienceIE demonstrate there are several potential systems that could be implemented to answer this problem, with the best system potentially being a combination of algorithms to select and label key phrases, either directly working together or separately and then their results being passed through some voting system to choose a final prediction. The majority of the solutions presented at ScienceIE involve supervised learning, which requires the use of the training data, with little mention of unsupervised approaches. This suggests the current best solutions require learning from examples, which could cause a problem if the training data is of poor quality or (as one team worked around) there a lack of examples for a given class. Furthermore, as training data is being used, over-fitting needs to be considered, where solutions presented may work well for the test set but not in the real world, but unfortunately it is hard to evaluate the application of the produced algorithms on random scientific articles as the researched ended after testing with ScienceIE data in most cases.

\section{Further Background}
Now a review of research in a larger field is reviewed to explore other contributions to information extraction.

A recent review of information extraction techniques was conducted in 2014 and listed various techniques \cite{Hasan2014}. A popular trend noted was that some systems begin by selecting candidate key phrases via some heuristic before applying any machine learning to the data, which may help reduce the problem complexity if less data needs to be processed. Supervised learning techniques included naïve Bayes, decision trees, bagging, boosting, maximum entropy, and SVMs. Unsupervised approaches used fields such as graph-based ranking, topic clustering, and language modelling to determine important words and phrases. 

Furthermore, the study also discussed evaluation of key phrases: It described the test model used at SemEval 2010 for a very similar key phrase extraction task to Science, and describes the matching between predicted information and gold standard information as \textit{overly strict}. It also discusses manual review, but this naturally is very expensive and time consuming to complete. By completing analysis of the errors produced by current key phrase extraction systems, it is described how over-generation (where the key phrase is predicted with extra information attached) is the largest problem at the moment. It does suggest a way to reduce this over-generation problem is to compare generated key phrases to a knowledge base to look for semantically equivalent pieces of information, to aid in the generation of more \textit{to the point} key phrases. ScienceIE's data could extend this further, as the use of hyponym and synonym extraction could be used to exploit the background knowledge about one key phrase to help verify another that there isn't a record found for.

A CRF based key phrase extraction model was completed for use on Chinese documents which saw very high results. Before describing their findings, this paper targets a language other than English, so their use of semantics may have different affects if applied in the same way to English sentences. The CRF model designed contained 22 different features, where almost all of the features were based around the position of the token in the text. For their test data set, the largest F1 score measured was 0.51, which is very similar to the best scoring mechanism used on the ScienceIE data set. This proves CRF models can be effective when applied to multiple languages, but also infers that little progress has been made in the decade since this research, given (although different data sets were used) very similar results. 

Naïve Bayes was used at ScienceIE to great effect, and has been used in the past effectively. A study that utilised naïve Bayes achieved good results by default, but were able to leverage information about the domain of the key phrases to increase their scores \cite{Wu2005}. Using information about the occurrence probabilities of key phrases throughout documents, based on the training set provided, this study shows that this basic information about the \textit{type} of key phrases can be integrated into an established classifier to boost scores. This study focussed on longer papers with the goal of producing a few key words per scientific paper, similar to that of an author. With the ScienceIE task however, the test data is short documents and they each can contain many key phrase (discussed in detail later). Therefore, any future system created should definitely consider at least the frequency of key phrases in documents, as there can be large differences in the expected key phrases for even the same documents.

As mentioned earlier, WordNet is also a potential source of information for building a classifier, although other knowledge bases exist. One study compared building a classifier using WordNet and Wikipedia for hyponym-only extraction \cite{Snow2004}. The concept of the system produced was that it could learn the lexical patterns of example hyponym pairs so that could be used to find hyponym relations. Using Wikipedia scored higher (F1 score of 0.36 on their dataset) than when using WordNet (F1 score of 0.27), showing it may be more suited to creating this type of classifier. While an improvement, it is not ultimately a huge increase and there is no evidence either Wikipedia is more useful for the specific area of scientific papers, as the dataset used in this study was not focussed. However, Wikipedia has more information that WordNet, and would likely cover more of the scientific concepts covered by any ScienceIE data. It is also updated on a more regular basis meaning new definitions are likely to be available much earlier than if using WordNet. With the study being completed some time ago, as Wikipedia matures it may increase or decrease the result of the experiment if repeated, depending upon the quality of the information contained in Wikipedia, which if tested would help evaluate its effectiveness.

A method not attempted at ScienceIE was unsupervised learning by clustering key phrases, a method which has potentially very accurate results that also could not only be robust again new unseen data but even different languages. A study used the idea that words can be clustered together through relatedness, with the goal of producing a set of clusters representing topics in a document \cite{Liu2009}. To extract the key phrases, using the simplest approach, the centre of a cluster is the key phrase. Various clustering methods were attempted by this study. They ran tests on relatively short articles and while at maximum they only achieved an F1 of ~0.45, there was several improvements suggested which apply to the ScienceIE task concerning scientific papers. Two suggested covered how the clusters were formed, including clustering on noun groups (which most key phrases at ScienceIE are) and creating a heuristic for co-occurrence based and Wikipedia based relatedness. Another suggestion was to improve the removal of unimportant words. The study only used stop words, but introducing a TF-IDF filter could also be possible.

% Continuing to discuss the clustering concept, there has been researching into combining SVM and clustering into a package for unsupervised learning \cite{Winters-Hilt2007}. An SVM can, effectively randomly, classify items into clusters. These clusters are then evaluated to see how well the items inside were classified. ** not really, and this isn't really useful at all...



\section{Word2Vec}
Through recommendations, this project also considers the use of Word2Vec. Word2Vec is an application of neural networks, that processes text and creates a vector space containing each word in the text. This vector space can be used to find the similarity and differences between words \cite{Mikolov2013}. This technology is relatively new (initially designed in 2013) and its uses are still being discovered, with a similar project called GloVe being worked on the following year \cite{Pennington2014}.

Given a large set of input texts, Word2Vec aims to learn the meaning of words. It processes all examples of where every word in the document library has been used contextually, and builds a vector representation of each word. These vector representations can be compared to try to extra the similarity, or relative similarity between words. For example, the relative distance from a \textit{knee} to a \textit{leg} may be similar to that of \textit{elbow} to \textit{arm}. It has been proven to be effective, although very difficult to understand \cite{Goldberg2014}. 

In terms of information extraction, there has been a recent study into its use to find extra hyponym information from text \cite{Nayak2015} using GloVe. This study designed a function based on the GloVe vector space to create a system to predict hyponym pairs. Reasonable results were achieved, scoring up to 35\% precision in some cases. However, a problem discussed is that words can have multiple meanings, which can effectively confuse the model, which decreases its quality for determining similarity and therefore reducing the effectiveness of a system built. 

\section{The Supplied Data Set}
The ScienceIE data set consists of 50 development, 350 training and 100 test documents. 

Some analysis conducted at ScienceIE \cite{Augenstein2017} showed some characteristics of the sample key phrases included:
\begin{itemize}
	\item Only 22\% of key phrases had 5 or more tokens,
	\item 93\% of key phrases were noun phrases,
	\item Only 31\% of key phrases seen in the training set were also in the test set.
\end{itemize}

This means that key phrase extraction appears quite difficult, as an algorithm needs to search for short phrases, processing phrases that it likely hasn't seen instances of before. Most of the key phrases being noun phrases, however, is valuable information as it helps to identify a simple heuristic that can be used when processing.

Other useful and interesting characteristics about the training set, found during this study, can be seen in table \ref{table:traininganalysis}. 

\begin{table}
	\centering
	\begin{tabular}{ c | c c c c }
		& \textbf{Minimum} & \textbf{Average} & \textbf{Maximum} & \textbf{Standard Deviation} \\
		\hline
		Nnumber of KPs & 4 & 19 & 46 & 8 \\
		Minimum tokens per KP & 1 & 1 & 3 & 0.4 \\
		Average tokens per KP & 1 & 3 & 8 & 1 \\
		Maximum tokens per KP & 2 & 9 & 25 & 4 \\
		Number of relations & 0 & 2 & 13 & 2 \\
		Total tokens in document & 60 & 159 & 264 & 46
	\end{tabular}
	\caption[ScienceIE Training Set Analysis]{Key phrase (KP), token and relation analysis for the ScienceIE training set.}
	\label{table:traininganalysis}
\end{table}

Papers in the ScienceIE data set have many key phrases associated with them. With an average of 19 key phrases per paper, an average of 3 tokens per key phrase (meaning on average 57 key tokens per paper) and the average document containing only 159 tokens in total, around a third of all tokens are part of key phrases. This is partly due to the documents supplied by ScienceIE being very short (all are just one paragraph) and are \textit{extracts} of papers rather than full publications. It is not a problem that the documents for processing are short - in fact that may help as the longer the document, the harder it is to choose key phrases \cite{Hasan2014} - however, it may mean any algorithm created here may not scale well to full scientific papers. It seems the ScienceIE task is looking for localised key phrases, choosing several from one paragraph; while the author of a paper may choose to select just five or ten key phrases from the whole paper. While this project will focus on the ScienceIE task with the given test data, a brief look longer or full papers shall be considered.
