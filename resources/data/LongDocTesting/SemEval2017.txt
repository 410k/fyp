Abstract
We describe the SemEval task of extract-ing keyphrases and relations between themfrom  scientific  documents,  which  is  cru-cial for understanding which publicationsdescribe  which  processes,  tasks  and  ma-terials.  Although this was a new task, wehad a total of 26 submissions across 3 eval-uation scenarios.  We expect the task andthe  findings  reported  in  this  paper  to  berelevant for researchers working on under-standing scientific content, as well as thebroader  knowledge  base  population  andinformation extraction communities.
Introduction
Empirical research requires gaining and maintain-ing an understanding of the body of work in spe-cific  area.For  example,  typical  questions  re-searchers  face  are  which  papers  describe  whichtasks and processes, use which materials and howthose  relate  to  one  another.   While  there  are  re-view  papers  for  some  areas,  such  information  isgenerally difficult to obtain without reading a largenumber of publications.Current efforts to address this gap are search en-gines such as Google Scholar,1Scopus2or Seman-tic Scholar,3which mainly focus on navigating au-thor and citations graphs.The  task  tackled  here  is  mention-level  iden-tification  and  classification  of  keyphrases,   e.g.KeyphraseExtraction (TASK), as well as extract-ing  semantic  relations  between  keywords,   e.g.KeyphraseExtractionHYPONYM-OFInforma-tionExtraction.    These  tasks  are  related  to  thetasks  of  named  entity  recognition,  named  entity lassification  and  relation  extraction.    However,keyphrases are much more challenging to identifythan  e.g.   person  names,  since  they  vary  signifi-cantly between domains, lack clear signifiers andcontexts and can consist of many tokens.  For thispurpose,  a  double-annotated  corpus  of  500  pub-lications with mention-level annotations was pro-duced, consisting of scientific articles of the Com-puter Science, Material Sciences and Physics do-mains.Extracting  keyphrases  and  relations  betweenthem is of great interest to scientific publishers asit  helps  to  recommend  articles  to  readers,  high-light missing citations to authors, identify poten-tial  reviewers  for  submissions,  and  analyse  re-search  trends  over  time.Note  that  organisingkeyphrases in terms of synonym and hypernym re-lations is particularly useful for search scenarios,e.g.  a reader may search for articles on informa-tion extraction, and through hypernym predictionwould also receive articles on named entity recog-nition or relation extraction.We expect the outcomes of the task to be rele-vant to the wider information extraction,  knowl-edge  base  population  and  knowledge  base  con-struction communities, as it offers a novel appli-cation domain for methods researched in that area,while still offering domain-related challenges.Since  the  dataset  is  annotated  for  three  tasksdependent  on  one  another,  it  could  also  be  usedas  a  testbed  for  joint  learning  or  structured  pre-diction approaches to information extraction (Kateand Mooney, 2010; Singh et al., 2013; Augensteinet al., 2015; Goyal and Dyer, 2016).Furthermore, we expect the task to be interest-ing for researchers studying tasks aiming at under-standing scientific content, such as keyphrase ex-traction (Kim et al., 2010b; Hasan and Ng, 2014;Sterckx  et  al.,  2016;  Augenstein  and  Søgaard,2017), semantic relation extraction (Tateisi et al., 2014;   Gupta   and   Manning,   2011;   Marsi   and ̈Ozt ̈urk, 2015), topic classification of scientific ar-ticles ( ́O S ́eaghdha and Teufel, 2014), citation con-text extraction (Teufel, 2006; Kaplan et al., 2009),extracting  author  and  citation  graphs  (Peng  andMcCallum, 2006; Chaimongkol et al., 2014; Simet al., 2015) or a combination of those (Radev andAbu-Jbara,  2012;  Gollapalli  and  Li,  2015;  Guoet al., 2015).The expected impact of the task is an interestof the above mentioned research communities be-yond the task due to the release of a new corpus,leading to novel research methods for informationextraction from scientific documents.   What willbe particularly useful about the proposed corpusare  annotations  of  hypernym  and  synonym  rela-tions on mention-level, as existing hypernym andsynonym relation resources are on type-level, e.g.WordNet.4Further, we expect that these methodswill directly impact industrial solutions to makingsense of publications, partly due to the task organ-isers’ collaboration with Elsevier.5
Task Description
he task is divided into three subtasks:A)  Mention-level keyphrase identificationB)  Mention-levelkeyphraseclassification.Keyphrase types arePROCESS(includingmethods,   equipment),TASKandMATE-RIAL(including corpora, physical materials)C)  Mention-level  semantic  relation  extractionbetween keyphrases with the same keyphrasetypes.  Relation types used areHYPONYM-OFandSYNONYM-OF.We will refer to the above subtasks as Subtask A,Subtask B, and Subtask C respectively.A  shortened  (artificial)  example  of  a  data  in-stance for the Computer Science area is displayedin Example 1, examples for Material Science andPhysics are included in the appendix. The first partis the plain text paragraph (with keyphrases in ital-ics  for  better  readability),  followed  by  stand-offkeyphrase annotations based on character offsets,followed relation annotations. Example 1.Text:Information  extractionis  the  process  ofextracting structured data from unstructured text,which is relevant for several end-to-end tasks, in-cludingquestion answering. This paper addressesthe tasks ofnamed entity recognition(NER), a sub-task ofinformation extraction,  usingconditionalrandom fields(CRF). Our method is evaluated ontheConLL-2003 NER corpus. 
3    Resources for SemEval-2017 Task
3.1    Corpus
A corpus for the task was built from ScienceDi-rect6open access publications and was availablefreely for participants, without the need to sign acopyright agreement.  Each data instance consistsof one paragraph of text, drawn from a scientificpaper.Publications were provided in plain text, in ad-dition to xml format, which included the full textof the publication as well as additional metadata.500  paragraphs  from  journal  articles  evenly  dis-tributed  among  the  domains  Computer  Science,Material Sciences and Physics were selected. The training data part of the corpus consists of350 documents,  50 for development and 100 fortesting.  This is similar to the pilot task describedin Section 5, for which 144 articles were used fortraining, 40 for development and for 100 testing.We  present  statistics  about  the  dataset  in  Ta-ble  1.   Notably,  the  dataset  contains  many  longkeyphrases.   22%  of  all  keyphrases  in  the  train-ing  set  consist  of  words  of  5  or  more  tokens.This contributes to making the task of keyphraseidentification very challenging.  However, 93% ofthose keyphrases are noun phrases7, which is valu-able information for simple heuristics to identifykeyphrase candidates.  Lastly, 31% of keyphrasescontained in the training dataset only appear in itonce,  systems  will  have  do  generalise  to  unseenkeyphrases well. 3.2    Annotation ProcessMention-level annotation is very time-consuming,and only a handful of semantic relations such ashypernymy and synonymy can be found in eachpublication.We  therefore  only  annotate  para-graphs of publications likely to contain relations.We originally intended to identify suitable doc-uments  by  automatically  extracting  a  knowledgegraph  of  relations  from  a  large  scientific  datasetusing  Hearst-style  patterns  (Hearst,  1991;  Snowet al., 2005), then using those to find potential re-lations  in  a  distinct  set  of  documents,  similar  tothe distant supervision (Mintz et al., 2009; Snowet  al.,  2005)  heuristic.   Documents  containing  ahigh  number  of  such  potential  relations  wouldthen  be  selected.    However,  this  requires  auto-matically learning to identify keyphrases betweenwhich those potential relations hold, and requiresrelations  to appear  several times  in  a dataset  forsuch a knowledge graph to be useful.In the end, this strategy was not feasible due tothe difficulty of learning to detect keyphrases au-tomatically and only a small overlap between rela-tions in different documents.  Instead, keyphrase-dense paragraphs were detected automatically us-ing  a  coarse  unsupervised  approach  (Mikolovet al., 2013) and those likely to contain relationswere selected manually for annotation.For  annotation,  undergraduate  student  volun-teers  studying  Computer  Science,  Material  Sci-ence or Physics were recruited using UCL’s student newsletter, which reaches all of its students.Students were shown example annotations and theannotation guidelines, and if they were still inter-ested  in  participating  in  the  annotation  exercise,afterwards asked to select beforehand how manydocuments  they  wanted  to  annotate.Approxi-mately 50% of students were still interested, hav-ing seen annotated documents and read annotationguidelines. They were then given two weeks to an-notate documents with the BRAT tool (Stenetorpet al., 2012), which was hosted on an Amazon EC2instance as a web service. Students were compen-sated  for  annotations  per  document.   Annotationtime was estimated as approximately 12 minutesper document and annotator, on which basis theywere paid roughly 10 GBP per hour.  They wereonly compensated upon completion of all annota-tions,  i.e.   compensation was conditioned on an-notating all documents.  The annotation cost wascovered by Elsevier. To develop annotation guide-lines, a small pilot annotation exercise on 20 doc-uments  was  performed  with  one  annotator  afterwhich annotation guidelines were refined.8We  originally  intended  for  student  annotatorsto  triple  annotate  documents  and  apply  majorityvoting  on  the  annotations,  but  due  to  difficultieswith recruiting high-quality annotators we insteadopted  to  double-annotate  documents,  where  thesecond annotator was an expert annotator.  Whereannotations  disagreed,  we  opted  for  the  expert’sannotation.Pairwise  inter-annotator  agreementbetween the student annotator and the expert anno-tator measured with Cohen’s kappa is shown in Ta-ble 2. The * indicates annotation quality decreasedover time, ending with the annotator not complet-ing annotating all documents. To account for this,documents for which no annotations are given areexcluded  from  computing  inter-annotator  agree-ment.   Out  of  the  annotators  completing  the  an-notation exercise, Cohen’s kappa ranges between0.45 and 0.85, with half of them having a substan-tial agreement of 0.6 or higher.  For future itera-tions of this task, we recommend to invest signifi-cant efforts into recruiting high-quality annotators,perhaps with more pre-annotation quality screen-ing.
4    Evaluation
SemEval 2017 Task 10 offers three different eval-uation scenarios:1)  Only plain text is given (Subtasks A, B, C).2)  Plain text with manually annotated keyphraseboundaries are given (Subtasks B, C).3)  Plaintextwithmanuallyannotatedkeyphrases    and    their    types    are    given(Subtask C).We refer to the above scenarios as Scenario 1, Sce-nario 2, and Scenario 3 respectively.4.1    MetricsKeyphrase  identification  (Subtask  A)  has  tradi-tionally  been  evaluated  by  calculating  the  ex-act  matches  with  the  gold  standard.There  isexisting  work  for  capturing  semantically  similarkeyphrases(Zesch  and  Gurevych,  2009;  Kim et al., 2010a), however since these are captured us-ing relations, similar to the pilot task on keyphraseextraction  (Section  5)  we  evaluate  keyphrases,keyphrase  types  and  relations  with  exact  matchcriteria.   The  output  of  systems  is  matched  ex-actly against the gold standard.  The traditionallyused metrics of precision, recall and F1-score arecomputed and the micro-average of those metricsacross publications of the three genres are calcu-lated.  These metrics are also calculated for Sub-tasks  B  and  C.  In  addition,  for  Subtasks  B  andC, participants are given the option of using textmanually annotated with keyphrase mentions andtypes.
5    Pilot Task
A pilot task on keyphrase extraction from scien-tific documents was run by other organisers at Se-mEval  2010  (Kim  et  al.,  2010b).   The  task  wasto  extract  a  list  of  keyphrases  representing  keytopics  from  scientific  documents,  i.e.   similar  tothe first part of our proposed Subtask A, only ontype-level.   Participants  were  allowed  to  submitup  to  3  runs  and  were  required  to  submit  a  listof  15  keyphrases  for  each  document,  ranked  bythe probability of being reader-assigned phrases.Data was collected from the ACM Digital Libraryfor the research areas Distributed Systems, Infor-mation  Search  and  Retrieval,  Distributed  Artifi-cial Intelligence   Multiagent Systems and Socialand Behavioral Sciences  Economics. Participantswere provided with 144 training, 40 developmentand  100  test  articles,  each  set  containing  a  mixof  articles  of  the  different  research  areas.    Thedata  was  provided  in  plain  text,  converted  frompdf withpdftotext.   Publications were annotatedwith keyphrases by 50 Computer Science studentsand added to author-provided keyphrases requiredby  the  journals  they  were  published  in.   Guide-lines  were  for  the  keyphrases  to  exactly  appear anywhere in the text of the paper, in reality 15%of annotator-provided keyphrases did not, as wellas 19% of author-provided keyphrases.  The num-ber of author-specified keywords was 4 on aver-age, whereas annotators identified 12 on average.Returned phrases are considered correct if they areexact matches of either the annotator- or author-assigned keyphrases, allowing for minor syntacticvariations (A of B→B A ; A’s B→A B). Preci-sion, recall and F1 is calculated for the top 5, top10 and all keywords.  19 systems were submittedto the task, the best one achieving an F1 of 27.5%on  the  combined  author-assigned  and  annotator-assigned keywords.Lessons learned from the task were that perfor-mance varies depending on how many keywordsare  to  be  extracted,  the  task  organisers  recom-mend against fixing a threshold for a number ofkeyphrases  to  extract  lead.   They  further  recom-mend a more semantically-motivated task, takinginto  account  synonyms  of  keyphrases  instead  ofrequiring exact matches. Both of those recommen-dations will be taken into account for future taskdesign.  To fulfill the latter, we will ask annotatorto assign types to the identified keywords (process,task, material) and identify semantic relations be-tween them (hypernym, synonym).
6    Existing Resources
As part of the FUSE project with IARPA, we cre-ated a small annotated corpus of 100 noun phrasesgenerated  from  the  titles  and  abstracts  derivedfrom the Web Of Science corpora9of the domainsPhysics, Computer Science, Chemistry and Com-puter Science. These corpora cannot be distributedpublicly and were made available by the IARPAfunding agency.  Annotation was performed by 3annotators using 14 fine-grained types, includingPROCESS.We measured inter-annotator agreement amongthe  three  annotators  for  the  14  categories  usingFleiss’ Kappa.  The k value was found to be 0.28which  implies  that  there  was  fair  agreement  be-tween them, however distinguishing between thefine-grained  types  added  significantly  to  the  an-notation time.  Therefore we only use three maintypes for the SemEval 2017 Task 10. There  are  some  existing  keyphrase  extractioncorpora, however, they are not similar enough tothe proposed task to justify reuse.  Below is a de-scription of existing corpora.The  SemEval  2010  Keyphrase  Extraction  cor-pus (Kim et al., 2010b)10consists of a handful ofdocument-level keyphrases per article. In contrastto the task proposed, the keyphrases are annotatedon type-level and not further classified as process,task or material and semantic relations are not an-notated.  Further, the domains considered are dif-ferent and mostly sub-domains of Computer Sci-ence.The  corpus  released  by  Tateisi  et  al.  (2014)11contains sentence-level fine-grained semantic an-notations for 230 publication abstracts in Japaneseand 400 in English.  In contrast to what we pro-pose,  the  annotations  are  more  fine-grained  andannotations are only available for abstracts.Gupta  and  Manning  (2011)  studied  keyphraseextraction from ACL Anthology articles, applyinga pattern-based bootstrapping approach based on15  016  documents  and  assigning  the  typesFO-CUS,TECHNIQUEandDOMAIN. Performancewas  evaluated  on  30  manually  annotated  docu-ments.   Although  the  latter  corpus  is  related  towhat we propose, manual annotation is only avail-able for a small number of documents and only forthe Natural Language Processing domain.The  ACL  RD-TEC  2.0  dataset  (QasemiZadehand  Schumann,  2016)  consists  of  300  ACL  An-thology abstracts annotated on mention-level withseven  different  types  of  keyphrases.   Unlike  ourdataset,  it  does  not  contain  relation  annotations.Note that this corpus was created at the same timeas the one SemEval 2017 Task 10 dataset and thuswe did not have the chance to build on it. A morein-depth comparison between the two datasets aswell as keyphrase identification and classificationmethods evaluated on them can be found in Au-genstein and Søgaard (2017).
6.1    Baselines
We frame the task as a sequence-to-sequence pre-diction task.  We preprocess the files by splittingdocuments  into  sentences  and  tokenising  themwith  nltk,  then  aligning  span  annotations  from.ann  files  to  tokens.   Each  sentence  is  regardedas one sequence.  We then split the task into the three subtasks, keyphrase boundary identification,keyphrase classification and relation classificationand add three output layers.   We predict the fol-lowing types, for the three subtasks respectively:Subtask A:tA=O, B, Ifor tokens being outside,at the beginning, or inside a keyphraseSubtask B:tB=O, M, P, Tfor tokens being out-side a keyphrase, or being part of a material, pro-cess or taskSubtask  C:tC=O, S, Hfor  Synonym-of  andHyponym-of relations.  For Subtask A and B, wepredict one output label per input token. For Sub-task  C  we  predict  a  vector  for  each  token,  thatencodes what the relationship between that tokenand  every  other  token  in  the  sequence  is  for  thefirst  token  in  each  keyphrase.   After  predictionsfor tokens are obtained, these are converted backto  spans  and  relations  between  them  in  a  post-processing step.We report results for two simple models: one toestimate theupper bound, that converts .ann filesinto instances,  as described above,  then convertsthem back into .ann files. Next, to estimate a lowerbound, arandom baseline, that for each token as-signs a random label for each of the subtasks.Theupper  boundspan-token-span  round-tripconversion performance, an F1 of 0.84, shows thatwe  already  lose  a  significant  amount  of  perfor-mance due to sentence splitting and tokenisationalone.   Therandombaseline  further  shows  hardespecially  the  keyphrase  boundary  identificationtask is and as a result the overall task,  since thesubtasks depend on one another.  For Subtask A,a  random  baseline  achieves  an  F1  of  0.03.   Theoverall  tasks  gets  easier  if  keyphrase  boundariesare  given,  resulting  in  F1  of  0.23  for  keyphraseclassification, and if keyphrase types are given, anF1 of 0.04 are achieved with the random baselinefor Subtask C.
7    Summary of Participating Systems
In  this  section,  we  summarise  the  outcome  ofthe  competition.For  more  details  please  re-fer  to  the  respective  system  description  papersand  the  task  websitehttps://scienceie.github.io/.We  had  three  subtasks,  described  in  Sec  2,which were grouped together in three evaluationscenarios,  described  in  Sec  4.   The  competitionwas  hosted  in  CodaLab12in  two  phases:  (i)  de-velopment phase and (ii) testing phase.  Fifty fourteams participated in the development phase, andout of them twenty six teams participated in the fi-nal competition.  One of the major success of thecompetition is due to such wide participation andapplication  of  various  different  techniques  start-ing  from  neural  networks,  supervised  classifica-tion with careful feature engineering to simple rulebased  methods.    We  present  a  summary  of  ap-proaches used by task participants below.
7.1    Evaluation Scenario 1
In this scenario teams need to solve all three sub-tasks A, B, and C; where no annotation informa-tion was given.  Some teams participated only inSubtask A, or B; but the overall micro F1 perfor-mance across subtasks is considered for the rank-ing of the teams.  Seventeen teams participated inthis scenario.   The F1 scores range from 0.04 to0.43. Complete results are given in Table 3.Various  different  types  of  methods  have  beenapplied by different teams with various levels ofsupervision.The  best  three  teams  TTICOIN,TIALUW,  and  s2end2end  have  used  recurrentneural  network  (RNN)  based  approaches  to  ob-tain F1 scores of 0.38, 0.42 and 0.43 respectively.However, TIALUW, and s2end2end, by using aconditional random  fields (CRF) layer on  top ofRNNs achieve a higher F1 in Subtask A comparedto TTICOIN.The fourth team PKUICL with an F1 of 0.37found classification models based on random for-est  and  support  vector  machines  (SVM)  usefulwith carefully engineered feature such as TF-IDFover  a  very  large  external  corpus,  IDF  weightedword-embeddings etc, along with an existing tax-onomy.  SciX on the other hand used noun phrasechunking  and  trained  an  SVM  classifier  on  pro-vided training data to classify phrases, and used aCRF to predict labels of the phrases.  CRF basedmethods with parts-of-speech (POS) tagging andorthographic features such as presence of symbolsand capitalisation have been tried by several teams(NTNU, SZTE-NLP, WING-NUS) and they lead-ing to a reasonable performance (F1:  0.23, 0.26,and 0.27, respectively).Noun  phrase  extraction  with  length  constraintby HCC-NLP, and using a global list of keyphrasesby  NITKITPG  are  found  not  to  perform  satis-factorily  (F1:   0.16  and  0.14  respectively).   The former  is  surprising,  as  keyphrases  are  with  anoverwhelming  majority  noun  phrases,  the  latternot as much, many keyphrases only appear oncein  the  dataset  (see  Table  1).   GMBUAP  furthertried  using  empirical  rules  obtained  by  observ-ing the training data for Subtask A, and a NaiveBayes classifier trained on provided training datafor Subtask B. Such simple methods on their ownprove not to be accurate enough. Attempts of suchgive us additional insight about the hardness of theproblem and applicability of simple methods to thetask.
7.2    Evaluation Scenario 2
In  this  scenario  teams  needed  to  solve  sub-tasksB, and C. Partial annotation was provided to theteams,  that  is,  solution  to  the  Subtask  A.  Fourteams participated in this scenario with F1 coresranging from 0.43 to 0.64.  Please refer to Table 4for complete result.Except  MayoNLP,  other  three  teams  partici-pated only in Subtask B. Although ranking is donebased on overall performance, but in this scenario ankings are consistent in each category.   BUAPwith  the  worst  F1  score  for  Subtask  B  (0.45),is  still  better  than  the  best  team  in  Scenario  1s2end2end for Subtask B (0.44).  Partial annota-tion or accuracy for Subtask A proves to be crit-ical,  reinforcing again that identifying keyphraseboundaries is the most difficult part of the sharedtask.Unlike  the  Scenario  1,  in  this  case  the  toptwo  teams  used  classifiers  with  lexical  features(F1:  0.64) as well as neural networks (F1:  0.63).The  first  team  MayoNLP  used  SVM  with  richfeature  sets  like  n-grams,   lexical  features,   or-thographic  features,   whereas  the  second  teamUKP/EELECTION used used three different neu-ral  network  approaches  and  subsequently  com-bined them via majority voting.  Both these meth-ods  perform  quite  similarly.However,  a  CRFbased  approach  and  an  SVM  with  simpler  fea-ture sets attempted by the two teams LABDA andBUAP are found to be less effective in this sce-nario.MayoNLP applied a simple rule based methodfor  synonym-of  relation  extraction,  and  Hearstpatterns  for  hyponym-of  relation  detection.   Therules for synonym-of detection is based on pres-ence of phrases such asin terms of,equivalently, which  are  calledetc  in  the  text  between  twokeyphrases.Interestingly,  the  RNN  based  ap-proach of s2end2end in Scenario 1 performs bet-ter than MayoNLP without using partial annota-tion of Subtask A.
7.3    Evaluation Scenario 3
In this scenario, teams need to solve only SubtaskC. Partial annotations were provided to the teamsfor  Subtask  B  and  C.  Five  teams  participated  inthis  scenario,  and  F1  scores  ranged  from  0.1  to0.64. Please refer to Table 5 for complete result.Neural network (NN) based models are found toperform better than other methods in this scenario.The best method by MIT uses a convolutional NN(CNN). The other method uses two phases of NNand found to be reasonably effective (F1: 0.54).On  the  other  hand,  application  of  supervisedclassification with five different classifiers (SVM,decision  tree,  random  forest,  multinomial  naive Bayes and k-nearest neighbour) using three differ-ent feature selection techniques (chi square, deci-sion tree, and recursive feature elimination) foundclose accuracy (F1:  0.5) with the top performingones.LaBDA also use a CNN based method.  How-ever, the rule based post-processing and argumentordering strategy applied by MIT seemed to giveadditional advantage as also observed by them.However most of the teams in this scenario out-perform, all teams from other scenarios (who didnot have access to partial information for SubtaskB, and C) in relation prediction.  This also assertsthe significance of accuracy on Subtask A, and Bin order to perform accurately on Subtask C.
8    Conclusion
In  this  paper,  we  present  the  setup  and  discussparticipating  systems  of  SemEval  2017  Task  10on identifying and classifying keyphrases and re-lations  between  them  from  scientific  articles,  towhich  26  systems  were  submitted.Successfulsystems vary in their approaches.   Most of themuse  RNNs,  often  in  combination  with  CRFs  aswell  as  CNNs,  however  the  system  performingbest for evaluation scenario 1 uses an SVM witha  well-engineered  lexical  feature  set.    Identify-ing  keyphrases  is  the  most  challenging  subtask,since  the  dataset  contains  many  long  and  infre-quent keyphrases, and systems relying on remem-bering them do not perform well.
