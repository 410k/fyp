\chapter{Background and Literature Review}

\section{Definitions and Descriptions}
Throughout this literature review there are several key natural language processing and machine learning concepts discussed. Rather than defining them as we arrive at them, a list of useful definitions is constructed here.

\subsubsection*{F1 Score}
The F1 score is a metric used to evaluate predictions. A common concept to find whe evaluating binary decisions is a \textit{confusion matrix} from which ROC analysis can be completed \cite{Fawcett2006}. This is a system which records \textit{true positive} and \textit{true negative} where the gold standard and predicted data match, and \textit{false positive} and \textit{false negative} where gold and predicted data do not match. From this, various values can be calculated. 

Accuracy is one, but often doesn't show the full story as if, on a data set where 9 out of 10 items are 'false' and just 1 item is 'true', predicting all false will get an accuracy of 90\%, but no \textit{true positive} occurrences will appear - which is bad. 

Two better metrics can be calculated, which are precision and recall. They look at the rates of correct and incorrect predictions, and can be combined together (and often are in the NLP world) to produce an F1 score. This is what ScienceIE's scripts calculate, given ScienceIE's gold standard data and a researchers predictions, comparing instances where the researcher has correctly predicted key phrase boundaries, classification and relations against the gold standard data.

An F1 score of 1 is perfect, and an F1 score of 0 means nothing was classified correctly at all.

\subsubsection*{Tokenization}
Tokenizatoin is a simple concept where a document is broken down, from one long string into individual words or symbols. 

\subsubsection*{Bag-Of-Words Representation}
Bag-of-Words is another simple concept, where each token is considered independently of is semantic meaning. 

\subsubsection*{Stop Words}
Stop words are words that are commonly filtered out due to their lack of specific meaning and generally do not contribute to useful input, although exceptions can be made if there is little other information. For example, common search engines are likely to ignore the word "the" in most searches, but this process of removal needs to be conducted carefully as, for example, when searching for "the who", "the" is an important part of that query. 

The stop words concept is used in this report, and the list of stop words used is taken from the Stanford CoreNLP GitHub repository\footnote{\href{https://github.com/stanfordnlp/CoreNLP/blob/master/data/edu/stanford/nlp/patterns/surface/stopwords.txt}{https://github.com/stanfordnlp/CoreNLP/blob/master/data/edu/stanford/nlp/patterns/surface/stopwords.txt}} and shown in appendix \ref{appendix:stopwords}.

\subsubsection*{Term Frequency - Inverse Document Frequency (TF-IDF)}
TF-IDF is a metric for assessing how important a word is in a piece of text and has many applications in NLP, including (which will be discussed later) document query \cite{Ramos2003}. It shall also see use throughout this report.

To be calculated for a given token, the document that token came from is required and a set of documents for comparison is required. The set of documents used in this project shall be the ScienceIE training set.

The theory is, a word like "the" - which will likely appear many times in almost every document - should a very low TF-IDF score (close to 0). Unusual words however, such as "xylanases" which are likely very specific to the paper they are contained in will probably have a much higher TF-IDF value than that of "the".

\subsubsection*{Parse Trees and Part-Of-Speech (POS) Tagging}
A \textit{parse} of a sentence is a tree structure where the root node is a \textit{sentence}, each node below that is a \textit{POS tag} and the leaves of the tree are the words or symbols in the sentence. The POS tag tells us about the semantic meaning of that node and its children - or sub tree - where the POS tag could be \textit{verb phrase} or \textit{noun phrase} for example.

\subsubsection*{Support Vector Machine (SVM)}
A SVM is a supervised machine learning mechanism. The input to a SVM is a series of vectors generated from some original input data. Each of these vectors is a set of features - which are simply values calculated based on the original input data. For training, these data points are labelled, indicating their class. Once trained, a SVM can be used to \textit{predict} the label for a new data point.
% todo a refernce here and improve in general

The training involves attempting to find a well fitting hyperplane with a maximal margin, that separates the labelled data, after mapping that data into a higher dimensional space. This involves an algorithm for finding the distance between the mapped data points, for which a \textit{kernel} can be specified. Furthermore, the hyperplane that is fitted can be allowed to make errors. This is where it allows training data points to be within the hyperplane margins (so may be miss classified if tested against). This can be tuned to increase SVM performance at the cost of run time increasing as well.

\subsubsection*{Clustering}
Clustering is simply the idea of grouping items together that are similar. The result should be a set of sets of items, where within the group there is a high average similarity, while inter-group similarity is much lower. This is often used for classification and there are a variety of clustering algorithms that can be applied, with various algorithms performing better for various applications \cite{Rai2010}.

\section{ScienceIE Proceedings}
Evaluating the outcome of ScienceIE at SemEval indicates potential paths for future systems and document very recent activity in the key phrase extraction area. Three papers were published from the event regarding this task.

Firstly, an overview of how successful the task was shall be conducted. The highest end-to-end F1 score achieved by any team was measured to be 0.43 for all three sub-systems combined \cite{Augenstein2017}, with each of subtasks A, B and C were 0.56, 0.44 and 0.28 respectively. 

For subtask A, it was evaluated that while many high scores were achieved with recurrent neural networks, the highest scoring system was a SVM using a well-engineered lexical feature set. SVMs and neural networks were also popular choices for subtask B. For subtask C, many methods were attempted and while a convolutional neural network was the most effective, various other methods (including SVM and multinomial naïve Bayes) all achieved very similar and reasonably accurate scores (the best had an F1 score of 0.64 when evaluated solely on subtask C).

-- Needs rewriting after this bit --

The best end-to-end ScienceIE team used a long short-term memory (LSTM) approach for phrase extraction, with labelling completed by a CRF based sequence tagging model \cite{Ammar2017}. Their sequence tagging model employed gazetteers built from scientific words extracted from the web. Another team \cite{Marsi2017} also had similar ideas, using CRFs to complete some of the task using WordNet\footnote{https://wordnet.princeton.edu/} as a data source for the classifier they created. Both teams here also used sensible rules to help improve their score, such as intuitively marking all instances of a key phrase as a key phrase upon finding one instance (so if \textit{carbon} is extracted and labelled as a \textit{material}, then all other instances are labelled to match) and exploiting hypernonym relationship’s bidirectional property (so if word 1 is a hypernym of word 2, the reverse is also true and therefore recorded). 

Unfortunately, while extraction and classification were generally well handled, relation extraction has very low accuracy across all teams taking part with the average F1 score only being 0.15, with the highest score being 0.28. \cite{Ammar2017} achieved this using gazetteer built from Wikipedia\footnote{https://en.wikipedia.org} and freebase. This seems more appropriate than hand written rules, which may seem appealing as they can be somewhat tailored and provide high accuracy, but require much more effort from the developer and may not work well on unseen conditions providing low accuracy \cite{Manning2012}. As mentioned earlier, WordNet is also a potential source of information for building a classifier for relation extraction, and a study by \cite{Snow2013} compared building a classifier off of Wordnet and Wikipedia for hypernym-only extraction. The result of this shows that Wikipedia may be more suited to creating this type of classifier as it achieved an F1 score higher than using WordNet (the Wikipedia based classifier got 0.36 while the WordNet based classifier got 0.27). While an improvement, it is not ultimately a huge increase and there is no evidence either Wikipedia is better for the specific area of scientific papers (as the 2013 study was completed on a generic set of data).

The results of ScienceIE demonstrate there are several potential systems that could be implemented to answer this problem, with the best system potentially being a combination of algorithms and a voting system to select and label key phrases. The product would likely involve supervised learning and previous knowledge for some algorithms, along with unsupervised learning sections as well.

\section{Other Revelant Background Information}

Several teams from ScienceIE chose to back up key phrase extraction with a CRF. CRFs can be used for key phrase extraction alone as well \cite{Zhang2008}, and while studies imply that CRFs (shown to have F1 scores of 0.51) are more accurate than an SVM (the most accurate at ScienceIE) this paper is slightly older than papers produced at SemEval 2017 and so even if the SVM information used then was the best that was available at the time (the SVM F1 score was 0.46), the SVM implemented at ScienceIE beat both of these scores considerably achieving an F1 of 0.56 as mentioned above.

A method not attempted at ScienceIE was unsupervised learning by clustering key phrases, a method which has potentially very accurate results that also could not only be robust again new unseen data but even different languages. The idea is that candidate key phrases are selected by some heuristic and other phrases are clustered about them. With the simplest approach, the center of a cluster is the key phrase. Various clustering methods were attempted by \cite{Liu2009} on top of a candidate selection process built on semantic term relatedness. They ran tests on relatively short articles and while at maximum they only achieved an F1 of ~0.45, there was several improvements suggested which apply to the task at hand concerning scientific papers. Firstly, an achievable improvement for this project would be to cluster directly on noun groups as they found most clusters consisted of groups of nouns anyway, which is backed up by Augenstein et al. \cite{Augenstein2017}, who reports 93\% of all key phrases are noun phrases. Furthermore, improving their initial filtering to extend it further than stop words may help reduce errors as well; improving this may be possible by employing a words TF-IDF score with some threshold. Finally, they suggested a similar algorithm be applied to longer scientific papers. ScienceIE’s test data consists of extracts of scientific texts (i.e. short paragraphs), however, any unsupervised system created for this task could be ran again entire papers and then only those sections compared for evaluation later – allowing this suggestion to be evaluated.

\section{Word2Vec}

\section{The Supplied Data Set}
The ScienceIE data set consists of 50 development, 350 training and 100 test documents. 

Some analysis conducted at ScienceIE \cite{Augenstein2017} showed some characteristics of the sample key phrases included:
\begin{itemize}
	\item Only 22\% of key phrases had 5 or more tokens,
	\item 93\% of key phrases were noun phrases,
	\item Only 31\% of key phrases seen in the training set were also in the test set.
\end{itemize}

This means that key phrase extraction appears quite difficult, as an algorithm needs to search for short phrases, processing phrases that it likely hasn't seen instances of before. Most of the key phrases being noun phrases, however, is valuable information as it helps to identify a simple heuristic that can be used when processing.

Other useful and interesting characteristics about the training set, found during this study, can be seen in table \ref{table:traininganalysis}. 

\begin{table}
	\centering
	\begin{tabular}{ c | c c c c }
		& \textbf{Minimum} & \textbf{Average} & \textbf{Maximum} & \textbf{Standard Deviation} \\
		\hline
		Nnumber of KPs & 4 & 19 & 46 & 8 \\
		Minimum tokens per KP & 1 & 1 & 3 & 0.4 \\
		Average tokens per KP & 1 & 3 & 8 & 1 \\
		Maximum tokens per KP & 2 & 9 & 25 & 4 \\
		Number of relations & 0 & 2 & 13 & 2 \\
		Total tokens in document & 60 & 159 & 264 & 46
	\end{tabular}
	\caption[ScienceIE Training Set Analysis]{Key phrase (KP), token and relation analysis for the ScienceIE training set.}
	\label{table:traininganalysis}
\end{table}

Papers in the ScienceIE data set have many key phrases associated with them. With an average of 19 key phrases per paper, an average of 3 tokens per key phrase (meaning on average 57 key tokens per paper) and the average document containing only 159 tokens in total, around a third of all tokens are part of key phrases. This is partly due to the documents supplied by ScienceIE being very short (all are just one paragraph) and are \textit{extracts} of papers rather than full publications. It is not a problem that the documents for processing are short - in fact that may help as the longer the document, the harder it is to choose key phrases \cite{Hasan2014} - however, it may mean any algorithm created here may not scale well to full scientific papers. It seems the ScienceIE task is looking for localised key phrases, choosing several from one paragraph; while the author of a paper may choose to select just five or ten key phrases from the whole paper. While this project will focus on the ScienceIE task with the given test data, a brief look longer or full papers shall be considered.
