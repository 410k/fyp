\chapter{Project Architecture}
With any large software project, it is sensible to choose a platform with all the developer is comfortable with and has a tool set relevant to the projects goal. The following describes the environment and technologies used generically through out the project.

\section{Language}
Due to the past experience of the author, Java was an obvious choice. Given extensive time working in the language during university and in industry, a thorough understanding of the programming language was already achieved, which allowed for planning of a sensible software architecture to optimise code quality and (implicitly) the potential of increased success of the systems created. 

Furthermore, Java is a very popular and accessible language world wide - backed up by the active StackOverflow community (casual and professional alike) for at least the last five years through their user surveys \footnote{2018 Survey: \href{https://insights.stackoverflow.com/survey/2018}{https://insights.stackoverflow.com/survey/2018}}\footnote{2017 Survey: \href{https://insights.stackoverflow.com/survey/2017}{https://insights.stackoverflow.com/survey/2017}}\footnote{2016 Survey: \href{https://insights.stackoverflow.com/survey/2016}{https://insights.stackoverflow.com/survey/2016}}. Due to this, Java has extensive support for many common problems, with issues being discussed and solutions proved across various forums. 

Not only is Java's popularity good for increasing support availability, many libraries and utilities are available to help developers with tasks. Along side other technologies used for more specific tasks throughout completion of the NLP and POC systems (which shall be discussed when used), common technologies used during the development of the entire project are described below. Throughout development of the project, very little issue was caused by lack of Java support for common processes or lack of Java capability when attempting to program some process (which was a critical part of evaluating which language should be used). 

Finally, Java serialisation was used throughout (and will be noted when is). Serialisation allows the system to save a Java object to disk (any file name can be chosen, but classically its postfix is \texttt{.ser}), and later be reloaded. 

As a brief aside, Python is another extremely popular language used for NLP and likely could have been used for at least the first half of this project producing similar results.

\subsection*{log4j}
log4j 2\footnote{\href{https://logging.apache.org/log4j/2.x/}{https://logging.apache.org/log4j/2.x/}} is a popular and robust library developed under the Apache Software Foundation to do logging in Java. It's useful features include (configured in \texttt{log4j2.xml}):
\begin{itemize}
	\item Automatic output of logs to both terminal and file for later review.
	\item Timing of logs which is very useful on long overnight process runs.
	\item Labelling of logs into levels such as \textit{debug}, \textit{info}, \textit{error} and \textit{fatal} messages. During development, all log levels aside from \textit{debug} displayed in terminal, so monitoring only saw the most important information, and then if need be the full logs with \textit{debug} could have been reviewed later.
\end{itemize}

While direct output of this will not be present in the rest of this report, it is worth noting this was an extremely useful tool for developing all of the systems to follow. 

\subsection*{Maven}
Apache Maven\footnote{\href{https://maven.apache.org/}{https://maven.apache.org/}} is another important tool, also developed by the Apache foundation. The tool helps with project management and has many uses. It is based around a \textit{project object model} (POM) configured in a \texttt{pom.xml} file at the root of a Java project. The key uses utilised in this project are:
\begin{itemize}
	\item Project compilation: Maven can be used to build a project and automatically run specified or all tests, with more detailed and well formatted output than compiling Java code (including importing libraries) by hand. This makes scripting and analysis easier.
	\item Library import: The POM can specify dependencies of the Java project. Maven has a central repository with many libraries available. This includes log4j described above, and all other libraries used in this project. Dependencies are downloaded to the systems local Maven repository at compile time.
	\item Library export: As discussed in the introduction, the NLP system shall be used in a POC system. Rather than combining these two systems into one large package, Maven can be used to export the compiled NLP system to the local Maven repository and then be included when building the POC system.
\end{itemize}

Maven is used as the management backbone throughout the development of software discussed in this report. When libraries are used in a project, a link to their dependency configuration for Maven's \texttt{pom.xml} shall be included. log4j's Maven configuration page\footnote{\href{https://logging.apache.org/log4j/2.x/maven-artifacts.html}{https://logging.apache.org/log4j/2.x/maven-artifacts.html}} is extensive and provides detailed descriptions of how to import the library.

\subsection*{JUnit}
JUnit is a popular Java framework for testing. Test methods are simply to declare with the \texttt{@Test} annotation, and can catch unexpected (or expected) exceptions and ensuring values are correct with \texttt{assert} statements. Somewhat unconventionally, there is a divide between JUnit tests in the NLP system: while some are based around ensuring functionality works as expected, many are actually building the NLP systems, training them, and testing their predictions.

Maven also integrates with it, so that when a project is built with Maven all of the methods in the test source directory are executed to ensure the program is working as expected, followed by a report. Maven will also automatically exclude the test files from the final packaged product to reduce waste space for deployments of projects. 

\subsection*{libsvm}
A popular SVM tool used in this project was libsvm (imported through Maven\footnote{\href{https://mvnrepository.com/artifact/com.datumbox/libsvm/3.22}{https://mvnrepository.com/artifact/com.datumbox/libsvm/3.22}}). Originally written in C and ported over to Java (as well as many other languages), libsvm was designed to be flexible, supporting various kernels and suitable for beginners through to advanced users. It has support for the core use of SVMs - training and predicting, but also has features to aid in parameter selection such as a cross-validation function, a visualiser for the training data and a data scaling tool \cite{ChangChih-ChungandLin2011}.

\subsection*{Word2Vec}
The interesting Word2Vec technology is utilised in this project in various places. The original Word2Vec library implementation was in Python. However, the Deep Learning For Java (DL4J) team have included, as part of their machine learning and deep neural network library, Word2Vec functionality\footnote{\href{https://deeplearning4j.org/word2vec.html}{https://deeplearning4j.org/word2vec.html}}. This supports training a Word2Vec model, using the model, and saving and loading models. 

The models used in this project are the Google News model\footnote{\href{https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing}{https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing}} and the Freebase model\footnote{\href{https://docs.google.com/file/d/0B7XkCwpI5KDYaDBDQm1tZGNDRHc/edit?usp=sharing}{https://docs.google.com/file/d/0B7XkCwpI5KDYaDBDQm1tZGNDRHc/edit?usp=sharing}}. While neither of these are made up of scientific articles, they both have a large vocabulary size (3 million and 1.4 million tokens respectively), and both based off of a 100 GB large samples, which should allow them to perform relatively well. This is the reason Word2Vec is being used over GloVe, as these models have much larger vocabulary sizes that the pre-trained GloVe models found, so in theory have a better change of covering the vocabulary of the ScienceIE data set\footnote{This GitHub page contains a selection of popular models for both Word2Vec and GloVe: \href{https://github.com/3Top/word2vec-api}{https://github.com/3Top/word2vec-api}}.

Attempts were made to use a Wikipedia based model (Wiki2Vec\footnote{\href{https://github.com/idio/wiki2vec}{https://github.com/idio/wiki2vec}}) but unfortunately no successful attempt was made to use it in this project (there were various problems converting the model to a Java readable format and loading it). While potentially of lower quality semantics (as Wikipedia can be edited by anyone) it may have had more of the vocabulary the ScienceIE data supports as Wikipedia covers many topics including those of scientific nature, so could have increased coverage of the model when finding similarities between various scientific tokens. 

\section{Platform}
While another benefit of Java is cross platform compatibility, some of the underlying system libraries that Wor2Vec relies on to function are included by default in many Linux distributions. It can be made to work on Microsoft Windows operating systems, but it requires a large amount of complex configuration and generally not worth the pay off. Therefore, this system was built on the Ubuntu 16.04 distribution of Linux, as this involved the least amount of configuration to get working.

Furthermore, some of the algorithms created as part of this paper are able to fill up available memory on a computer. The memory available as part of this project was 16 GB. Linux swap space was configured (an \textit{overflow} area for memory usage) but generally one would not like to use this, as it is slow to read, dramatically increasing algorithm run time. This is another reason for using Linux as the memory overhead from the operating system is much smaller when compared with Microsoft Windows 10 (in the order of gigabytes of memory saved). Furthermore, Linux can also be run headless to further reduce the operating systems memory usage, which on Ubuntu 16.04 saves approximately an extra gigabyte of memory. With support for Secure Shell (SSH) to remotely connect to the system to run tests, Linux is an excellent choice of platform for optimising memory usage while running these algorithms.
